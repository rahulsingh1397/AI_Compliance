"""
NLP Model for Data Discovery Agent.

This module implements NLP models (BERT, spaCy) for unstructured data classification
to identify sensitive information like PII in emails, documents, etc.

Features:
- Chunked text processing for long documents
- Enhanced PII detection with regex patterns and spaCy
- BERT-based classification with configurable chunk size and stride
"""

import os
import re
import spacy
import numpy as np
import logging
import warnings
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import pandas as pd
import torch
from pydantic import BaseModel, Field
from tqdm import tqdm
from transformers import (
    AutoModelForSequenceClassification,
    AutoModelForTokenClassification,
    AutoTokenizer,
    pipeline,
)
try:
    from datasets import Dataset
    DATASETS_AVAILABLE = True
except ImportError:
    DATASETS_AVAILABLE = False
    warnings.warn("'datasets' package not found, falling back to standard processing. Install with: pip install datasets")

# Suppress HuggingFace warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings('ignore', message=".*Using a non-full backward hook*")
warnings.filterwarnings('ignore', message=".*You seem to be using the pipelines sequentially on GPU.*")
warnings.filterwarnings("ignore", message="`return_all_scores` is now deprecated")

# Configure logging
logger = logging.getLogger(__name__)

# Default chunking parameters
DEFAULT_CHUNK_SIZE = 512  # BERT's maximum sequence length
DEFAULT_STRIDE = 256      # Overlap between chunks

class NLPModel:
    """
    NLP Model for sensitive data identification and classification.
    Uses BERT for document-level classification and spaCy for entity recognition.
    """
    
    def __init__(self, 
                 bert_classifier_model_name: str = "distilbert-base-uncased-finetuned-sst-2-english",
                 bert_ner_model_name: str = "dslim/bert-base-NER", 
                 spacy_model_name: str = "en_core_web_sm",
                 device: Optional[Union[int, str]] = None,
                 chunk_size: int = DEFAULT_CHUNK_SIZE,
                 stride: int = DEFAULT_STRIDE):
        """
        Initialize NLP models for data classification with enhanced PII detection.
        
        Args:
            bert_classifier_model_name: Name or path of the BERT model for text classification.
            bert_ner_model_name: Name or path of the BERT model for Named Entity Recognition (NER).
            spacy_model_name: Name of the spaCy model to use for basic NLP tasks.
            device: Device to run models on (None for auto, 'cuda' for GPU, 'cpu' for CPU, or device index e.g., 0).
            chunk_size: Maximum number of tokens per chunk for BERT processing.
            stride: Number of overlapping tokens between chunks.
        """
        # Configure device
        if device is None:
            self.device_id = 0 if torch.cuda.is_available() else -1
        elif isinstance(device, str) and device.lower() == 'cuda':
            self.device_id = 0 if torch.cuda.is_available() else -1 # Default to GPU 0 if available
        elif isinstance(device, str) and device.lower() == 'cpu':
            self.device_id = -1
        elif isinstance(device, int):
            self.device_id = device
        else:
            logger.warning(f"Invalid device specified: {device}. Defaulting to auto-detection.")
            self.device_id = 0 if torch.cuda.is_available() else -1

        logger.info(f"Initializing NLPModel on device: {'cuda:' + str(self.device_id) if self.device_id != -1 else 'cpu'}")

        # Initialize BERT tokenizer and model for Text Classification
        logger.info(f"Loading BERT classification model: {bert_classifier_model_name}")
        
        # Determine if we can use FP16 precision for better performance
        use_fp16 = torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 7
        torch_dtype = torch.float16 if use_fp16 else None
        
        if use_fp16:
            logger.info("Using FP16 precision for better GPU performance")
        
        self.classifier_model = AutoModelForSequenceClassification.from_pretrained(
            bert_classifier_model_name, 
            torch_dtype=torch_dtype
        )
        self.classifier_tokenizer = AutoTokenizer.from_pretrained(bert_classifier_model_name)
        
        # Get device ID for pipeline
        self.device_id = 0 if torch.cuda.is_available() else -1
        
        # Initialize the text classifier pipeline with FP16 support if available
        self.text_classifier_pipeline = pipeline(
            "text-classification",
            model=self.classifier_model,
            tokenizer=self.classifier_tokenizer,
            device=self.device_id,
            return_all_scores=True,
            framework="pt",
            torch_dtype=torch_dtype  # Enable FP16 if supported by GPU
        )

        # Determine the positive label for the classifier model
        try:
            if hasattr(self.classifier_model, 'config') and hasattr(self.classifier_model.config, 'id2label') and 1 in self.classifier_model.config.id2label:
                self.positive_label_str_lower = self.classifier_model.config.id2label[1].lower()
                logger.info(f"Using '{self.positive_label_str_lower}' as the primary sensitive/positive label for classification.")
            else:
                # This case might occur if the model is not binary or id2label is structured differently
                logger.warning("Could not reliably determine positive label (id 1) from model config. Defaulting to 'positive'.")
                self.positive_label_str_lower = "positive"
        except Exception as e: # Catch any other unexpected errors during config access
            logger.error(f"Error determining positive label from model config: {e}. Defaulting to 'positive'.")
            self.positive_label_str_lower = "positive"
        
        # Define keywords that indicate a sensitive classification, including the determined positive label and a generic 'sensitive' term
        self.sensitive_label_keywords = {self.positive_label_str_lower, "sensitive"}

        # Initialize BERT tokenizer and model for NER
        logger.info(f"Loading BERT NER model: {bert_ner_model_name}")
        self.ner_tokenizer = AutoTokenizer.from_pretrained(bert_ner_model_name, use_fast=True)
        self.ner_model = AutoModelForTokenClassification.from_pretrained(bert_ner_model_name)
        self.ner_pipeline = pipeline(
            "ner", 
            model=self.ner_model,
            tokenizer=self.ner_tokenizer, 
            device=self.device_id,
            aggregation_strategy="simple" # Groups sub-word tokens
        )

        # Initialize spaCy model
        logger.info(f"Loading spaCy model: {spacy_model_name}")
        try:
            # First try loading without disabling any components
            self.nlp = spacy.load(spacy_model_name)
            
            # Check if sentencizer is already in the pipeline
            if 'sentencizer' not in self.nlp.pipe_names:
                try:
                    # Add sentencizer at the beginning of the pipeline
                    self.nlp.add_pipe('sentencizer', first=True)
                    logger.info("Added 'sentencizer' to the beginning of the spaCy pipeline.")
                except Exception as e_sent:
                    logger.error(f"Could not add 'sentencizer' to spaCy pipeline: {e_sent}")
                    # Fallback to basic chunking
                    logger.warning("Falling back to basic chunking without sentence boundaries.")
            else:
                logger.info("Found 'sentencizer' in the spaCy pipeline.")
                
            # Now disable components we don't need to save memory
            for pipe_name in ["parser", "ner", "textcat"]:
                if pipe_name in self.nlp.pipe_names:
                    self.nlp.disable_pipe(pipe_name)
                    logger.debug(f"Disabled spaCy pipeline component: {pipe_name}")
                    
            # Verify sentencizer is still enabled
            if 'sentencizer' not in self.nlp.pipe_names:
                logger.warning("Sentencizer was disabled. Re-adding it to the pipeline.")
                try:
                    self.nlp.add_pipe('sentencizer', first=True)
                except Exception as e:
                    logger.error(f"Failed to re-add sentencizer: {e}")
                    
        except OSError:
            logger.warning(f"spaCy model '{spacy_model_name}' not found. Attempting to download...")
            from spacy.cli import download as spacy_download
            try:
                spacy_download(spacy_model_name)
                self.nlp = spacy.load(spacy_model_name)
                
                # Add sentencizer if not present after download
                if 'sentencizer' not in self.nlp.pipe_names:
                    try:
                        self.nlp.add_pipe('sentencizer', first=True)
                        logger.info("Added 'sentencizer' after downloading spaCy model.")
                    except Exception as e_dl_sent:
                        logger.error(f"Could not add sentencizer after download: {e_dl_sent}")
                
                # Disable unnecessary components
                for pipe_name in ["parser", "ner", "textcat"]:
                    if pipe_name in self.nlp.pipe_names:
                        self.nlp.disable_pipe(pipe_name)
                        logger.debug(f"Disabled downloaded model's pipeline component: {pipe_name}")
                        
                logger.info(f"Successfully downloaded and loaded spaCy model: {spacy_model_name}")
                
            except Exception as e_dl:
                logger.error(f"Failed to download or initialize spaCy model: {e_dl}")
                self.nlp = None
        except Exception as e:
            logger.error(f"Error initializing spaCy: {e}")
            self.nlp = None
            
        # If spaCy still couldn't be loaded, log a warning and proceed without it
        if not self.nlp:
            logger.warning("spaCy could not be loaded. Some features may be limited.")
        
        # Configure chunking parameters
        self.chunk_size = chunk_size
        self.stride = stride
        
        # Define PII entity types and their regex patterns (if applicable)
        # Confidence scores here are base scores, can be adjusted by source (BERT, spaCy, regex)
        self.pii_definitions = {
            "PERSON": {"label": "Person Name", "regex": None, "confidence": 0.85},
            "EMAIL": {"label": "Email Address", "regex": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b", "confidence": 0.98},
            "PHONE_NUMBER": {"label": "Phone Number", "regex": r"\b(?:\+?\d{1,3}[-.\s]?)?(?:\(\d{3}\)|\d{3})[-.\s]?\d{3}[-.\s]?\d{4}\b", "confidence": 0.95},
            "US_SSN": {"label": "US Social Security Number", "regex": r"\b\d{3}[- ]?\d{2}[- ]?\d{4}\b", "confidence": 0.99},
            "CREDIT_CARD": {"label": "Credit Card Number", "regex": r"\b(?:\d[ -]*?){13,19}\b", "confidence": 0.97}, # General pattern, specific cards below
            "VISA": {"label": "Credit Card (Visa)", "regex": r"\b4[0-9]{12}(?:[0-9]{3})?\b", "confidence": 0.98},
            "MASTERCARD": {"label": "Credit Card (Mastercard)", "regex": r"\b(?:5[1-5][0-9]{2}|222[1-9]|22[3-9][0-9]|2[3-6][0-9]{2}|27[01][0-9]|2720)[0-9]{12}\b", "confidence": 0.98},
            "AMEX": {"label": "Credit Card (American Express)", "regex": r"\b3[47][0-9]{13}\b", "confidence": 0.98},
            "DISCOVER": {"label": "Credit Card (Discover)", "regex": r"\b6(?:011|5[0-9]{2})[0-9]{12}\b", "confidence": 0.98},
            "ADDRESS": {"label": "Physical Address", "regex": r"\b\d+\s+([A-Za-z0-9\s,.-]+)(\b(?:Apt|Suite|Unit|#)[\s.]*\w+)?\s*,?\s*[A-Za-z\s]+(?:,\s*[A-Z]{2})?\s*\d{5}(?:-\d{4})?\b", "confidence": 0.75},
            "DATE_OF_BIRTH": {"label": "Date of Birth", "regex": r"\b(?:(?:0?[1-9]|1[0-2])[-/\s](?:0?[1-9]|[12][0-9]|3[01])[-/\s](?:19|20)?\d{2})|(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*[-.\s]+\d{1,2}(?:st|nd|rd|th)?[\s.,-]+(?:19|20)?\d{2})\b", "confidence": 0.80},
            "IP_ADDRESS": {"label": "IP Address", "regex": r"\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b", "confidence": 0.99},
            "PASSPORT_NUMBER": {"label": "Passport Number", "regex": r"\b[A-PR-WYa-pr-wy][1-9]\d{7}[A-PR-WYa-pr-wy]?\b", "confidence": 0.90}, # Common patterns, vary by country
            "DRIVERS_LICENSE": {"label": "Driver's License", "regex": r"\b[A-Za-z][0-9]{6,12}\b", "confidence": 0.85}, # Highly variable
            "BANK_ACCOUNT_NUMBER": {"label": "Bank Account Number", "regex": r"\b\d{6,17}\b", "confidence": 0.70},
            # spaCy specific labels we might want to map or use
            "ORG": {"label": "Organization", "regex": None, "confidence": 0.70},
            "GPE": {"label": "Geopolitical Entity", "regex": None, "confidence": 0.70},
            "DATE": {"label": "Date", "regex": None, "confidence": 0.70} # General date, distinct from DOB
        }

        # Initialize EntityRuler and add it to the spaCy pipeline
        if not self.nlp.has_pipe("entity_ruler"):
            # Try to add before 'ner' if 'ner' exists, otherwise add at the end.
            # overwrite_ents = True allows custom entities to overwrite existing ones.
            if self.nlp.has_pipe("ner"):
                self.ruler = self.nlp.add_pipe("entity_ruler", before="ner", config={"overwrite_ents": True})
            else:
                self.ruler = self.nlp.add_pipe("entity_ruler", config={"overwrite_ents": True})
            logger.info("Added EntityRuler to spaCy pipeline.")
        else:
            self.ruler = self.nlp.get_pipe("entity_ruler") # Get existing ruler
            logger.info("EntityRuler already exists in spaCy pipeline. Using existing.")

        self._add_custom_patterns_to_spacy()
        logger.info("NLPModel initialization complete.")
    
    def _add_custom_patterns_to_spacy(self):
        """
        Adds custom PII detection patterns (from self.pii_patterns) to the spaCy EntityRuler.
        """
        if not hasattr(self, 'ruler') or self.ruler is None:
            logger.error("EntityRuler (self.ruler) not initialized. Cannot add custom patterns.")
            return

        patterns = []
        # Corrected to use self.pii_definitions
        for pii_type_key, attributes in self.pii_definitions.items(): 
            if attributes.get("regex") and attributes.get("label"):
                # spaCy's EntityRuler pattern format for regex:
                # The 'label' for the EntityRuler should be the PII type key (e.g., "EMAIL", "US_SSN")
                # so we can map it back easily using pii_definitions.
                pattern = {
                    "label": pii_type_key,  # Use the PII type key as the spaCy label
                    "pattern": [{ "TEXT": { "REGEX": attributes["regex"] } }],
                    "id": pii_type_key  # Use the original key as an ID for the pattern
                }
                patterns.append(pattern)
            elif attributes.get("regex"):
                logger.warning(f"Skipping PII pattern for '{pii_type_key}' due to missing 'label'. Regex: {attributes['regex']}")
            # We don't warn if regex is missing because some PII types are only for spaCy/BERT NER (e.g., PERSON)
        
        if patterns:
            try:
                self.ruler.add_patterns(patterns)
                logger.info(f"Added {len(patterns)} custom regex PII patterns to spaCy EntityRuler.")
            except Exception as e:
                logger.error(f"Error adding patterns to EntityRuler: {e}", exc_info=True)
        else:
            logger.info("No valid regex-based PII patterns found in pii_definitions to add to spaCy EntityRuler.")
    
    def _chunk_text_by_sentences(self, text: str) -> List[str]:
        """
        Splits text into chunks by sentences, ensuring no chunk exceeds the max token limit.
        Args:
            text: Text to chunk
            
        Returns:
            List of text chunks respecting sentence boundaries when possible
        """
        if not self.nlp or not self.nlp.has_pipe('sentencizer'):
            logger.warning("spaCy sentencizer not available or not functional. Falling back to basic chunking.")
            return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]
            
        # Use spaCy to split text into sentences
        doc = self.nlp(text)
        sentences = [sent.text for sent in doc.sents]
        
        # Use the standalone tokenizer instead of pipeline.tokenizer to avoid "Already borrowed" errors
        tokenizer = self.classifier_tokenizer  # Use the dedicated tokenizer, not pipeline.tokenizer
        
        chunks = []
        current_chunk_sents = []
        current_chunk_len = 0
        
        # Build chunks by accumulating sentences until we reach token limit
        for sent in sentences:
            sent_tokens = tokenizer.encode(sent, add_special_tokens=False)
            sent_len = len(sent_tokens)
            
            if current_chunk_len + sent_len > self.chunk_size - 2:  # Leave room for special tokens
                if current_chunk_sents:
                    chunks.append(" ".join(current_chunk_sents))
                current_chunk_sents = [sent]
                current_chunk_len = sent_len
            else:
                current_chunk_sents.append(sent)
                current_chunk_len += sent_len
                
        # Add the last chunk if any sentences remain
        if current_chunk_sents:
            chunks.append(" ".join(current_chunk_sents))
            
        return chunks
    
    def _process_batch(self, batch: List[str]) -> List[Dict]:
        """Process a batch of text chunks through the classifier pipeline with token length control."""
        try:
            # Ensure each chunk is within token limits
            processed_batch = []
            for chunk in batch:
                tokens = self.classifier_tokenizer.encode(chunk, add_special_tokens=True)
                if len(tokens) > self.chunk_size:
                    logger.debug(f"Truncating chunk from {len(tokens)} to {self.chunk_size} tokens")
                    tokens = tokens[:self.chunk_size]
                    chunk = self.classifier_tokenizer.decode(tokens[:-1])  # Remove partial token
                processed_batch.append(chunk)
            
            logger.debug(f"Processing batch of {len(processed_batch)} chunks")
            
            # Process batch with proper padding and truncation
            return self.text_classifier_pipeline(
                processed_batch,
                truncation=True,
                padding=True,
                max_length=self.chunk_size,
                batch_size=8,
                top_k=None  # Replaces return_all_scores=True
            )
        except Exception as e:
            logger.error(f"Error processing batch: {e}", exc_info=True)
            return []
    

    def _safe_truncate_text(self, text: str, max_length: int = None) -> str:
        """
        Safely truncate text to a maximum token length without
        risking 'Already borrowed' errors.
        
        Args:
            text: Text to truncate
            max_length: Maximum token length (defaults to self.chunk_size)
            
        Returns:
            Truncated text that will fit within token limit
        """
        if not text:
            return text
            
        # Use a much more conservative max_length
        if max_length is None:
            max_length = min(self.chunk_size, 512)  # Never exceed 512 tokens
        else:
            max_length = min(max_length, 512)  # Enforce hard limit of 512
        
        # Much more conservative character estimation (2.5 chars per token)
        # BERT tokenizers often use subword units, so a single word could be multiple tokens
        approx_chars_per_token = 2.5  # Conservative estimate
        
        # Always truncate to be safe if over 70% of the limit
        target_char_limit = int(max_length * approx_chars_per_token * 0.7)
        
        if len(text) > target_char_limit:
            logger.debug(f"Aggressive truncation from {len(text)} chars to {target_char_limit} chars")
            return text[:target_char_limit]  # Very aggressive truncation
            
        return text
    
    def _process_chunks_in_batches(self, chunks: List[str]) -> List[Dict]:
        """
        Process all text chunks in optimized batches using multiprocessing and Hugging Face datasets.map()
        for better performance on multi-core systems.
        
        Args:
            chunks: List of text chunks to classify
            
        Returns:
            List of classification results for each chunk
        """
        if not chunks:
            return []
            
        logger.info(f"Processing {len(chunks)} chunks using multicore batch processing approach")
        
        # CRITICAL: Ensure no chunk exceeds token limit using safe truncation method
        # that doesn't risk "Already borrowed" errors
        processed_chunks = [self._safe_truncate_text(chunk) for chunk in chunks]
        
        # Process using Dataset to leverage multicore processing
        try:
            if DATASETS_AVAILABLE:
                # Create a simple dataset with pre-truncated texts
                hf_dataset = Dataset.from_dict({"text": processed_chunks})
                
                # Determine optimal number of CPU cores to use (leave some cores free)
                import multiprocessing
                total_cores = multiprocessing.cpu_count()
                optimal_cores = max(1, min(total_cores - 1, 4))  # Use at most 4 cores, leave 1 free
                logger.info(f"Using {optimal_cores} CPU cores for parallel processing (out of {total_cores} available)")
                
                # Create a proper function to enable caching
                def predict_text_batch(examples):
                    # Explicitly use truncation and conservative max_length
                    output = {"results": []}
                    try:
                        # Use a more controlled approach with extremely conservative truncation
                        output["results"] = self.text_classifier_pipeline(
                            examples["text"],
                            truncation=True, 
                            padding=True,
                            max_length=512,  # Hard limit at 512 tokens
                        )
                    except Exception as e:
                        logger.error(f"Error in batch prediction: {e}")
                    return output
                
                # Use batched processing with map with multiprocessing
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore")
                    results_dataset = hf_dataset.map(
                        predict_text_batch,
                        batched=True,
                        batch_size=4,  # Smaller batch size for stability
                        num_proc=optimal_cores  # Use multiple CPU cores
                    )
                
                # Extract results while handling potential missing values
                if "results" in results_dataset.column_names:
                    logger.info(f"Successfully processed {len(chunks)} chunks in parallel")
                    return results_dataset["results"]
                else:
                    logger.error("No 'results' column found in processed dataset")
                    return []
            else:
                # Fallback to multiprocessing pool for direct processing
                logger.info("Datasets library not available, using multiprocessing.Pool for parallel processing")
                from multiprocessing import Pool, cpu_count
                
                # Determine optimal number of CPU cores
                optimal_cores = max(1, min(cpu_count() - 1, 4))
                batch_size = 4  # Reasonable batch size
                all_results = []
                
                # Prepare batches for processing
                batches = [processed_chunks[i:i+batch_size] for i in range(0, len(processed_chunks), batch_size)]
                
                # Define a worker function for parallel processing
                def process_batch(batch):
                    try:
                        return self.text_classifier_pipeline(
                            batch,
                            truncation=True,
                            padding=True,
                            max_length=512  # Hard limit at 512 tokens
                        )
                    except Exception as batch_err:
                        logger.error(f"Error processing batch: {batch_err}")
                        # Return empty placeholder results
                        return [[{"label": "UNKNOWN", "score": 0.0}]] * len(batch)
                
                # Process batches in parallel
                logger.info(f"Processing {len(batches)} batches across {optimal_cores} CPU cores")
                with Pool(processes=optimal_cores) as pool:
                    batch_results = pool.map(process_batch, batches)
                
                # Flatten results
                for result in batch_results:
                    all_results.extend(result)
                    
                logger.info(f"Parallel processing complete. Processed {len(all_results)} chunks.")
                return all_results
        except Exception as e:
            logger.error(f"Critical error in batch processing: {e}", exc_info=True)
            return []
    
    def classify_document(self, text: str) -> Dict[str, Any]:
        """
        Classify document to determine if it contains sensitive information using efficient batch processing.
        
        Args:
            text: Document text to classify.
            
        Returns:
            Dictionary with classification results (contains_sensitive_data, confidence, classification).
        """
        if not text or not text.strip():
            logger.info("Empty text provided for classification. Returning non-sensitive.")
            return {
                "contains_sensitive_data": False,
                "confidence": 0.0,
                "classification": "non-sensitive",
                "error": "Empty input text",
                "chunk_count": 0
            }

        # Chunk the document to handle long texts
        chunks = self._chunk_text_by_sentences(text)
        if not chunks:
            logger.warning("No text chunks were generated for classification.")
            return {
                'contains_sensitive_data': False, 
                'confidence': 0.0, 
                'classification': 'No Content',
                'chunk_count': 0
            }

        # Process all chunks in optimized batches
        all_results = self._process_chunks_in_batches(chunks)
        
        # Parse results
        classifications = []
        confidences = []
        
        for result in all_results:
            if not result or not isinstance(result, list) or not result:
                continue
            
            # Get highest scoring label for this chunk
            try:
                top_label = max(result, key=lambda x: x['score'])
                predicted_label = top_label['label'].lower()
                score = top_label['score']
                
                is_sensitive = predicted_label in self.sensitive_label_keywords
                classifications.append(is_sensitive)
                confidences.append(score)
            except Exception as e:
                logger.warning(f"Could not determine classification from result: {e}")
                continue

        # Determine overall classification based on chunk results
        if not classifications:
            logger.warning("No chunks were successfully classified. Returning default non-sensitive.")
            return {
                "contains_sensitive_data": False,
                "confidence": 0.0,
                "classification": "non-sensitive",
                "chunk_count": len(chunks),
                "error": "No chunks successfully classified"
            }

        contains_sensitive_data = any(classifications)
        
        # Calculate confidence based on classification result
        if contains_sensitive_data:
            # Use maximum confidence from sensitive chunks
            relevant_confidences = [conf for i, conf in enumerate(confidences) if classifications[i]]
            final_confidence = max(relevant_confidences) if relevant_confidences else 0.0
        else:
            # Use minimum confidence from all chunks (weakest link)
            final_confidence = min(confidences) if confidences else 0.0
            
        return {
            "contains_sensitive_data": contains_sensitive_data,
            "confidence": float(final_confidence),
            "classification": "sensitive" if contains_sensitive_data else "non-sensitive",
            "chunk_count": len(chunks)
        }
        
    def detect_pii(self, text: str) -> List[Dict[str, Any]]:
        """
        Detect PII entities in text using multiple NER approaches (BERT, spaCy).
        
        Args:
            text: Input text to analyze for PII entities
            
        Returns:
            List of PII entities with type, value, position, and confidence
        """
        if not text or not text.strip():
            logger.info("Empty text provided for PII detection.")
            return []
            
        collected_entities = []
        chunks = []
        
        # 1. BERT NER Processing (with chunking)
        if self.ner_pipeline is not None:
            try:
                logger.info("Starting BERT NER processing")
                # Chunk the text for BERT processing
                chunks = self._chunk_text_by_sentences(text)
                logger.info(f"Text split into {len(chunks)} chunks for BERT NER processing")
                
                # Map from BERT NER entity types to our PII definitions
                bert_to_pii_map = {
                    # Standard BERT NER types
                    'PER': 'PERSON',
                    'PERSON': 'PERSON',
                    'B-PER': 'PERSON', 
                    'I-PER': 'PERSON',
                    'ORG': 'ORG',
                    'B-ORG': 'ORG',
                    'I-ORG': 'ORG',
                    'LOC': 'GPE',
                    'B-LOC': 'GPE',
                    'I-LOC': 'GPE',
                    'LOCATION': 'GPE',
                    'GPE': 'GPE',
                    'MISC': 'MISC',
                    'B-MISC': 'MISC',
                    'I-MISC': 'MISC',
                    # Add other mappings as needed
                }
                
                # Process each chunk with NER
                current_doc_search_offset = 0
                for i, chunk in enumerate(chunks):
                    current_chunk_text = chunk
                    
                    try:
                        # Process the chunk with BERT NER
                        chunk_bert_entities = self.ner_pipeline(current_chunk_text)
                        logger.debug(f"BERT NER found {len(chunk_bert_entities)} entities in chunk {i+1}/{len(chunks)}")
                        
                    except Exception as e:
                        logger.error(f"Error processing chunk {i+1} with BERT NER: {e}", exc_info=True)
                        continue
                        
                    # Find the actual offset of this chunk in the original text
                    # This is necessary because chunking might change the exact text
                    # Find actual chunk start offset in the document
                    chunk_actual_start_in_doc = text.find(current_chunk_text, current_doc_search_offset)

                    if chunk_actual_start_in_doc == -1:
                        # Fallback: if not found from search offset, try from beginning
                        chunk_actual_start_in_doc = text.find(current_chunk_text)
                        if chunk_actual_start_in_doc == -1:
                            logger.warning(f"Could not locate chunk {i+1}/{len(chunks)} in original text. Skipping entities for this chunk.")
                            logger.warning(f"Unknown BERT NER entity format (no entity type): {bert_entity}")
                            continue
                            
                        # Extract score with fallbacks
                        if 'score' in bert_entity:
                            entity_score = bert_entity['score']
                        elif 'confidence' in bert_entity:
                            entity_score = bert_entity['confidence']
                        else:
                            logger.warning(f"No confidence score in entity: {bert_entity}, using default 0.7")
                            entity_score = 0.7  # Default confidence
                            
                        # Extract text value with fallbacks
                        if 'word' in bert_entity:
                            entity_word = bert_entity['word']
                        elif 'text' in bert_entity:
                            entity_word = bert_entity['text']
                        else:
                            logger.warning(f"No word/text in entity: {bert_entity}, will extract from position")
                            
                        # Extract position with fallbacks
                        if 'start' in bert_entity and 'end' in bert_entity:
                            entity_start = bert_entity['start']
                            entity_end = bert_entity['end']
                        elif 'start_pos' in bert_entity and 'end_pos' in bert_entity:
                            entity_start = bert_entity['start_pos']
                            entity_end = bert_entity['end_pos']
                        else:
                            logger.warning(f"No position info in entity: {bert_entity}, skipping")
                            continue
                                    
                        # Skip 'O' (Outside) entities
                        if entity_type == 'O':
                            continue
                            
                        pii_key = bert_to_pii_map.get(entity_type)
                        if pii_key and pii_key in self.pii_definitions:
                            definition = self.pii_definitions[pii_key]
                            entity_abs_start = chunk_actual_start_in_doc + entity_start
                            entity_abs_end = chunk_actual_start_in_doc + entity_end
                            
                            # Get entity value either from the model or from the text using position
                            if entity_word and chunk_actual_start_in_doc >= 0:
                                entity_value = entity_word
                            else:
                                entity_value = text[entity_abs_start:entity_abs_end]
                                
                            entity_data = {
                                "type": definition["label"],
                                "value": entity_value,
                                "start": entity_abs_start,
                                "end": entity_abs_end,
                                "confidence": definition["confidence"],
                                "model_score": float(entity_score),
                                "source": "bert_ner"
                            }
                            logger.debug(f"BERT NER detected {entity_data['type']}: '{entity_data['value']}' with confidence {entity_data['model_score']:.2f}")
                            collected_entities.append(entity_data)
                    # Advance search offset for the next chunk
                    current_doc_search_offset = chunk_actual_start_in_doc + 1 # Start search for next chunk after beginning of this one
            except Exception as e:
                logger.error(f"Error during BERT NER processing: {e}", exc_info=True)
                    
                    for bert_entity in chunk_bert_entities:
                        # Handle different NER model output formats
                        entity_type = None
                        entity_score = 0.0
                        entity_word = None
                        entity_start = 0
                        entity_end = 0
                        
                        # Extract entity type with fallbacks for different model outputs
                        if 'entity_group' in bert_entity:
                            entity_type = bert_entity['entity_group']
                        elif 'entity' in bert_entity:
                            entity_type = bert_entity['entity']
                        elif 'type' in bert_entity:
                            entity_type = bert_entity['type']
                        else:
                            logger.warning(f"Unknown BERT NER entity format (no entity type): {bert_entity}")
                            continue
                            
                        # Extract score with fallbacks
                        if 'score' in bert_entity:
                            entity_score = bert_entity['score']
                        elif 'confidence' in bert_entity:
                            entity_score = bert_entity['confidence']
                        else:
                            logger.warning(f"No confidence score in entity: {bert_entity}, using default 0.7")
                            entity_score = 0.7  # Default confidence
                            
                        # Extract text value with fallbacks
                        if 'word' in bert_entity:
                            entity_word = bert_entity['word']
                        elif 'text' in bert_entity:
                            entity_word = bert_entity['text']
                        else:
                            logger.warning(f"No word/text in entity: {bert_entity}, will extract from position")
                            
                        # Extract position with fallbacks
                        if 'start' in bert_entity and 'end' in bert_entity:
                            entity_start = bert_entity['start']
                            entity_end = bert_entity['end']
                        elif 'start_pos' in bert_entity and 'end_pos' in bert_entity:
                            entity_start = bert_entity['start_pos']
                            entity_end = bert_entity['end_pos']
                        else:
                            logger.warning(f"No position info in entity: {bert_entity}, skipping")
                            continue
                                    
                        # Skip 'O' (Outside) entities
                        if entity_type == 'O':
                            continue
                            
                        pii_key = bert_to_pii_map.get(entity_type)
                        if pii_key and pii_key in self.pii_definitions:
                            definition = self.pii_definitions[pii_key]
                            entity_abs_start = chunk_actual_start_in_doc + entity_start
                            entity_abs_end = chunk_actual_start_in_doc + entity_end
                            
                            # Get entity value either from the model or from the text using position
                            if entity_word and chunk_actual_start_in_doc >= 0:
                                entity_value = entity_word
                            else:
                                entity_value = text[entity_abs_start:entity_abs_end]
                                
                            entity_data = {
                                "type": definition["label"],
                                "value": entity_value,
                                "start": entity_abs_start,
                                "end": entity_abs_end,
                                "confidence": definition["confidence"],
                                "model_score": float(entity_score),
                                "source": "bert_ner"
                            }
                            logger.debug(f"BERT NER detected {entity_data['type']}: '{entity_data['value']}' with confidence {entity_data['model_score']:.2f}")
                            collected_entities.append(entity_data)
                    # Advance search offset for the next chunk
                    current_doc_search_offset = chunk_actual_start_in_doc + 1 # Start search for next chunk after the beginning of this one
            else:
                logger.info("No chunks generated for BERT NER processing.")
        else:
            logger.info("BERT NER pipeline not available. Skipping BERT PII detection.")

        # 2. spaCy NER Processing (Custom Patterns + Default NER)
        if self.nlp is not None:
            try:
                logger.info("Starting spaCy NER processing")
                
                # Check if the spaCy model has necessary components
                if not self.nlp.has_pipe("ner"):
                    logger.warning("spaCy model missing NER component. Entity detection will be limited.")
                    
                # Process with spaCy (consider using multi-processing for large text)
                doc = self.nlp(text)
                
                # Count entities for logging
                entity_count = len(doc.ents)
                logger.info(f"spaCy identified {entity_count} entities in the text")
                
                # Add standard regex patterns for common PII formats if few entities found
                if entity_count < 2:
                    logger.info("Few entities detected by spaCy, adding regex pattern matching")
                    # Run regex matching for common patterns (emails, phone numbers, SSNs)
                    text_lower = text.lower()
                    
                    # Simple regex for email detection
                    email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                    for match in re.finditer(email_pattern, text):
                        entity_data = {
                            "type": "EMAIL",
                            "value": match.group(),
                            "start": match.start(),
                            "end": match.end(),
                            "confidence": 0.9,  # High confidence for regex pattern match
                            "source": "regex_pattern"
                        }
                        logger.info(f"Regex detected EMAIL: '{entity_data['value']}'")
                        collected_entities.append(entity_data)
                    
                    # Simple regex for phone number detection (US format)
                    phone_pattern = r'\(\d{3}\)\s*\d{3}[-\s]?\d{4}|\d{3}[-\s]?\d{3}[-\s]?\d{4}'
                    for match in re.finditer(phone_pattern, text):
                        entity_data = {
                            "type": "PHONE_NUMBER",
                            "value": match.group(),
                            "start": match.start(),
                            "end": match.end(),
                            "confidence": 0.9,
                            "source": "regex_pattern"
                        }
                        logger.info(f"Regex detected PHONE_NUMBER: '{entity_data['value']}'")
                        collected_entities.append(entity_data)
                
                # Process entities identified by spaCy
                for ent in doc.ents:
                    pii_key = None
                    # Debug each entity
                    logger.debug(f"spaCy entity found: {ent.text} - Label: {ent.label_}")
                    
                    # ent.label_ could be a direct key from pii_definitions or a standard spaCy label
                    if ent.label_ in self.pii_definitions:
                        pii_key = ent.label_
                    else:
                        # Enhanced mapping of standard spaCy labels
                        spacy_label_upper = ent.label_.upper()
                        if spacy_label_upper in ['PERSON', 'PER'] and 'PERSON' in self.pii_definitions: 
                            pii_key = 'PERSON'
                        elif spacy_label_upper in ['ORG', 'ORGANIZATION'] and 'ORG' in self.pii_definitions: 
                            pii_key = 'ORG'
                        elif spacy_label_upper in ['GPE', 'LOC', 'FAC', 'LOCATION'] and 'GPE' in self.pii_definitions: 
                            pii_key = 'GPE'
                        elif spacy_label_upper in ['DATE', 'TIME'] and 'DATE' in self.pii_definitions: 
                            pii_key = 'DATE'
                        elif 'MISC' in self.pii_definitions and ent.text.strip():
                            # Catch-all for any other entity type that has text
                            pii_key = 'MISC'

                    if pii_key:
                        definition = self.pii_definitions[pii_key]
                        
                        # Use a lower confidence threshold for initial detection
                        # We'll filter by confidence later if needed
                        confidence_threshold = 0.6  # Lower threshold to catch more potential entities
                        
                        entity_data = {
                            "type": definition["label"],
                            "value": ent.text,
                            "start": ent.start_char,
                            "end": ent.end_char,
                            "confidence": max(confidence_threshold, definition.get("confidence", 0.6)),
                            "source": "spacy_ner"
                        }
                        logger.info(f"spaCy detected {entity_data['type']}: '{entity_data['value']}' with confidence {entity_data['confidence']:.2f}")
                        collected_entities.append(entity_data)
                
                # If still no entities found, try a different approach
                if not collected_entities:
                    logger.warning("No entities detected with standard spaCy processing, attempting alternative approaches")
                    # Log some text sample for debugging
                    sample = text[:200] + '...' if len(text) > 200 else text
                    logger.debug(f"Sample text being analyzed: '{sample}'")
                    
            except Exception as e:
                logger.error(f"Error during spaCy NER processing: {e}", exc_info=True)
        else:
            logger.info("spaCy model not available. Skipping spaCy PII detection.")

        # 3. Deduplication and consolidation with lower thresholds
        logger.info(f"Starting deduplication and filtering of {len(collected_entities)} raw PII detections")
        
        # Lower the confidence threshold for initial collection to ensure we catch more entities
        minimum_confidence = 0.5  # Lower threshold to catch more potential entities
        
        # Filter by minimum confidence but be lenient
        collected_entities = [e for e in collected_entities if 
                             e.get('model_score', e.get('confidence', 0)) >= minimum_confidence]
        
        logger.info(f"After confidence filtering ({minimum_confidence}), {len(collected_entities)} entities remain")
        
        # Sort by start position and then by length (longer matches first)
        collected_entities.sort(key=lambda e: (e['start'], -(e['end'] - e['start'])))
        
        unique_entities_tracker = set()
        final_pii_entities = []
        
        # First pass: Remove exact duplicates but be more lenient with what's considered a duplicate
        for entity in collected_entities:
            # Normalize the entity value by removing excess whitespace and lowercasing
            normalized_value = ' '.join(entity['value'].lower().split())
            
            # Create a more flexible entity key that doesn't require exact position matches
            # This helps catch entities that might be detected slightly differently by different models
            entity_key = (normalized_value, entity['type'])
            
            # For exact position duplicates, use a more stringent key
            exact_key = (normalized_value, entity['start'], entity['end'], entity['type'])
            
            if exact_key not in unique_entities_tracker:
                # For fuzzy matching, we look at the value and type, not position
                # But we need to check if this entity overlaps with any existing one
                is_duplicate = False
                
                for existing in final_pii_entities:
                    existing_norm = ' '.join(existing['value'].lower().split())
                    
                    # Check for significant overlap in text or position
                    text_similar = normalized_value in existing_norm or existing_norm in normalized_value
                    position_overlap = (entity['start'] < existing['end'] and entity['end'] > existing['start'])
                    
                    if text_similar and position_overlap and entity['type'] == existing['type']:
                        is_duplicate = True
                        
                        # If duplicate but higher confidence, replace the existing one
                        entity_conf = entity.get('model_score', entity.get('confidence', 0))
                        existing_conf = existing.get('model_score', existing.get('confidence', 0))
                        
                        if entity_conf > existing_conf:
                            final_pii_entities.remove(existing)
                            final_pii_entities.append(entity)
                            logger.debug(f"Replaced {existing['type']}: '{existing['value']}' with higher confidence entity: '{entity['value']}'")
                        break
                
                if not is_duplicate:
                    final_pii_entities.append(entity)
                    unique_entities_tracker.add(exact_key)  # Track exact matches
        
        logger.info(f"After deduplication, {len(final_pii_entities)} unique entities remain")
        
        # Enhanced logging of remaining entities
        if final_pii_entities:
            logger.debug("Remaining PII entities after deduplication:")
            for i, entity in enumerate(final_pii_entities, 1):
                conf = entity.get('model_score', entity.get('confidence', 0))
                logger.debug(f"  {i}. {entity['type']}: '{entity['value']}' (Conf: {conf:.2f}, Start: {entity['start']}, End: {entity['end']})")

        
        # Log detailed information about detected PII
        if final_pii_entities:
            logger.info(f"Detected {len(final_pii_entities)} unique PII entities:")
            for i, entity in enumerate(final_pii_entities, 1):
                source = entity.get('source', 'unknown')
                conf = entity.get('model_score', entity.get('confidence', 1.0))
                logger.info(f"  {i}. {entity['type']}: '{entity['value']}' (Confidence: {conf:.2f}, Source: {source})")
        else:
            logger.info("No PII entities detected in the provided text")
            
            # If no PII was found but we had chunks, log a sample of the text for debugging
            if chunks and len(chunks) > 0:
                sample_text = chunks[0][:200] + '...' if len(chunks[0]) > 200 else chunks[0]
                logger.debug(f"Sample of text being analyzed (first chunk): {sample_text}")
        
        return final_pii_entities
    
    def analyze_document(self, text: str) -> Dict[str, Any]:
        """
        Perform full document analysis for sensitive data using multicore parallelism.
        
        Args:
            text: The input text to analyze for sensitive information.
            
        Returns:
            A dictionary containing:
            - classification: Dictionary with classification results
            - pii_entities: List of detected PII entities
            - pii_count: Number of PII entities found
            - sensitivity_score: Confidence score of the classification
            - chunk_count: Number of chunks processed
            - processing_stats: Dictionary with processing statistics
        """
        import time
        start_time = time.time()
        
        # Process classification and PII detection in parallel if text is large enough
        # to benefit from parallel processing (avoids overhead for small texts)
        if len(text) > 5000:  # Only parallelize for larger texts
            logger.info(f"Large text detected ({len(text)} chars), using parallel processing")
            
            from concurrent.futures import ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=2) as executor:
                # Submit both tasks to be executed in parallel
                classify_future = executor.submit(self.classify_document, text)
                pii_future = executor.submit(self.detect_pii, text)
                
                # Get results as they complete
                classification = classify_future.result()
                pii_entities = pii_future.result()
        else:
            # For smaller texts, sequential is more efficient
            classification = self.classify_document(text)
            pii_entities = self.detect_pii(text)
        
        # Calculate processing statistics
        processing_time = time.time() - start_time
        chars_per_second = len(text) / processing_time if processing_time > 0 else 0
        
        # Enhanced return information including stats
        result = {
            "classification": classification,
            "pii_entities": pii_entities,
            "pii_count": len(pii_entities),
            "sensitivity_score": classification["confidence"],
            "chunk_count": classification.get("chunk_count", 1),
            "processing_stats": {
                "text_length": len(text),
                "processing_time_seconds": round(processing_time, 3),
                "chars_per_second": int(chars_per_second),
                "parallel_processing": len(text) > 5000
            }
        }
        
        # Log detailed results
        logger.info(f"Document analysis complete in {processing_time:.2f}s. "
                   f"Found {len(pii_entities)} PII entities with sensitivity score {classification['confidence']:.2f}")
        
        return result
